{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ahmad17/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import ir_datasets\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = expand_contractions(text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assuming the necessary imports and NLTK downloads are already done as shown in the previous example\n",
    "\n",
    "def prepare_dataset(dataset,output_prefix):\n",
    "    docs = [preprocess(doc.text) for doc in dataset.docs_iter()]\n",
    "    docs_ids = [doc.doc_id for doc in dataset.docs_iter()]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    num_clusters = 10 \n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    docs_df = pd.DataFrame({'doc_id': docs_ids, 'text': docs, 'clusters': cluster_labels})\n",
    "    docs_df.to_csv(f'{output_prefix}/docs_df.csv', index=False)\n",
    "    joblib.dump(tfidf_matrix, f'{output_prefix}/tfidf_matrix.pkl')\n",
    "    joblib.dump(vectorizer, f'{output_prefix}/vectorizer.pkl')\n",
    "    joblib.dump(kmeans, f'{output_prefix}/kmeans.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T run the following 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "antique_dataset = ir_datasets.load(\"antique/train\")\n",
    "prepare_dataset(antique_dataset, \"models/antique\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] If you have a local copy of https://zenodo.org/record/3565761/files/wikIR1k.zip, you can symlink it here to avoid downloading it again: /home/ahmad17/.ir_datasets/downloads/554299bca984640cb283d6ba55753608\n",
      "[INFO] [starting] https://zenodo.org/record/3565761/files/wikIR1k.zip\n",
      "[INFO] [finished] https://zenodo.org/record/3565761/files/wikIR1k.zip: [01:54] [165MB] [1.45MB/s]\n",
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "wiki_dataset = ir_datasets.load(\"wikir/en1k/training\")\n",
    "prepare_dataset(wiki_dataset, \"models/wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_search(query, tfidf_matrix, vectorizer, top_n=10):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return top_indices, cosine_similarities[top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_clustering(query, tfidf_matrix, vectorizer, kmeans, top_n=10):\n",
    "    query_vec = vectorizer.transform([preprocess(query)])\n",
    "    top_cluster = kmeans.predict(query_vec)[0]\n",
    "    cluster_indices = np.where(kmeans.labels_ == top_cluster)[0]\n",
    "    cluster_similarities = cosine_similarity(query_vec, tfidf_matrix[cluster_indices]).flatten()\n",
    "    cluster_top_indices = cluster_indices[cluster_similarities.argsort()[-top_n:][::-1]]\n",
    "    return cluster_top_indices, cluster_similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_tfidf_search(queries, qrels, tfidf_matrix, vectorizer, docs_df):\n",
    "    average_precisions = []\n",
    "    reciprocal_ranks = []\n",
    "    precision_at_10 = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for query_id, query_text in queries.items():\n",
    "        relevant_docs = {doc_id for doc_id, relevance in qrels.get(query_id, []) if relevance > 0}\n",
    "\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        top_indices, _ = tfidf_search(query_text, tfidf_matrix, vectorizer, top_n=10)\n",
    "        retrieved_docs = set(docs_df.iloc[top_indices].doc_id)\n",
    "\n",
    "        # Calculate Precision@10\n",
    "        top_10_retrieved = set(docs_df.iloc[top_indices[:10]].doc_id)\n",
    "        precision_10 = len(top_10_retrieved & relevant_docs) / 10.0\n",
    "        precision_at_10.append(precision_10)\n",
    "\n",
    "        # Calculate Recall\n",
    "        recall = len(relevant_docs & retrieved_docs) / len(relevant_docs)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "        # Calculate Average Precision\n",
    "        num_relevant = 0\n",
    "        sum_precisions = 0.0\n",
    "        for i, doc_id in enumerate(docs_df.iloc[top_indices].doc_id):\n",
    "            if doc_id in relevant_docs:\n",
    "                num_relevant += 1\n",
    "                sum_precisions += num_relevant / (i + 1.0)\n",
    "        if num_relevant > 0:\n",
    "            average_precisions.append(sum_precisions / num_relevant)\n",
    "\n",
    "        # Calculate Reciprocal Rank\n",
    "        rr = 0.0\n",
    "        for i, doc_id in enumerate(docs_df.iloc[top_indices].doc_id):\n",
    "            if doc_id in relevant_docs:\n",
    "                rr = 1.0 / (i + 1.0)\n",
    "                break\n",
    "        reciprocal_ranks.append(rr)\n",
    "\n",
    "    MAP = np.mean(average_precisions) if average_precisions else 0.0\n",
    "    MRR = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "    mean_precision_10 = np.mean(precision_at_10) if precision_at_10 else 0.0\n",
    "    mean_recall = np.mean(recall_scores) if recall_scores else 0.0\n",
    "\n",
    "    return MAP, MRR, mean_precision_10, mean_recall\n",
    "\n",
    "def evaluate_tfidf_search_with_clustering(queries, qrels, tfidf_matrix, vectorizer, docs_df, kmeans):\n",
    "    average_precisions = []\n",
    "    reciprocal_ranks = []\n",
    "    precision_at_10 = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for query_id, query_text in queries.items():\n",
    "        relevant_docs = {doc_id for doc_id, relevance in qrels.get(query_id, []) if relevance > 0}\n",
    "\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        top_indices, _ = search_with_clustering(query_text, tfidf_matrix, vectorizer, kmeans, top_n=10)\n",
    "        retrieved_docs = set(docs_df.iloc[top_indices].doc_id)\n",
    "\n",
    "        # Calculate Precision@10\n",
    "        top_10_retrieved = set(docs_df.iloc[top_indices[:10]].doc_id)\n",
    "        precision_10 = len(top_10_retrieved & relevant_docs) / 10.0\n",
    "        precision_at_10.append(precision_10)\n",
    "\n",
    "        # Calculate Recall\n",
    "        recall = len(relevant_docs & retrieved_docs) / len(relevant_docs)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "        # Calculate Average Precision\n",
    "        num_relevant = 0\n",
    "        sum_precisions = 0.0\n",
    "        for i, doc_id in enumerate(docs_df.iloc[top_indices].doc_id):\n",
    "            if doc_id in relevant_docs:\n",
    "                num_relevant += 1\n",
    "                sum_precisions += num_relevant / (i + 1.0)\n",
    "        if num_relevant > 0:\n",
    "            average_precisions.append(sum_precisions / num_relevant)\n",
    "\n",
    "        # Calculate Reciprocal Rank\n",
    "        rr = 0.0\n",
    "        for i, doc_id in enumerate(docs_df.iloc[top_indices].doc_id):\n",
    "            if doc_id in relevant_docs:\n",
    "                rr = 1.0 / (i + 1.0)\n",
    "                break\n",
    "        reciprocal_ranks.append(rr)\n",
    "\n",
    "    MAP = np.mean(average_precisions) if average_precisions else 0.0\n",
    "    MRR = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "    mean_precision_10 = np.mean(precision_at_10) if precision_at_10 else 0.0\n",
    "    mean_recall = np.mean(recall_scores) if recall_scores else 0.0\n",
    "\n",
    "    return MAP, MRR, mean_precision_10, mean_recall\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "antique_tfidf_matrix = joblib.load(\"./models/antique/tfidf_matrix.pkl\")\n",
    "antique_vectorizer = joblib.load(\"./models/antique/vectorizer.pkl\")\n",
    "antique_kmeans = joblib.load(\"./models/antique/kmeans.pkl\")\n",
    "antique_docs_df = pd.read_csv(\"./models/antique/docs_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tfidf_matrix = joblib.load(\"./models/wiki/tfidf_matrix.pkl\")\n",
    "wiki_vectorizer = joblib.load(\"./models/wiki/vectorizer.pkl\")\n",
    "wiki_kmeans = joblib.load(\"./models/wiki/kmeans.pkl\")\n",
    "wiki_docs_df = pd.read_csv(\"./models/wiki/docs_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "antique_dataset = ir_datasets.load(\"antique/train\")\n",
    "antique_queries = {query.query_id: query.text for query in antique_dataset.queries_iter()}\n",
    "antique_qrels = {}\n",
    "for qrel in antique_dataset.qrels_iter():\n",
    "    if qrel.query_id not in antique_qrels:\n",
    "        antique_qrels[qrel.query_id] = []\n",
    "    antique_qrels[qrel.query_id].append((qrel.doc_id, qrel.relevance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dataset = ir_datasets.load(\"wikir/en1k/training\")\n",
    "wiki_queries = {query.query_id: query.text for query in wiki_dataset.queries_iter()}\n",
    "wiki_qrels = {}\n",
    "for qrel in wiki_dataset.qrels_iter():\n",
    "    if qrel.query_id not in wiki_qrels:\n",
    "        wiki_qrels[qrel.query_id] = []\n",
    "    wiki_qrels[qrel.query_id].append((qrel.doc_id, qrel.relevance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DON'T run the following 4 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TF-IDF System ( Antique ) ...\n",
      "TF-IDF Results: (0.14731879736223874, 0.08120193050879908, 0.0304204451772465, 0.11765890734283808)\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating TF-IDF System ( Antique ) ...\")\n",
    "tfidf_antique_results = evaluate_tfidf_search(antique_queries, antique_qrels, antique_tfidf_matrix, antique_vectorizer, antique_docs_df)\n",
    "print(f\"TF-IDF Results: {tfidf_antique_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating clustered TF-IDF System ( Antique ) ...\n",
      "clustered TF-IDF Results: (0.5787161922272746, 0.2408775958858399, 0.07061005770816159, 0.08247039539032426)\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating clustered TF-IDF System ( Antique ) ...\")\n",
    "antique_clustered_results = evaluate_tfidf_search_with_clustering(antique_queries, antique_qrels, antique_tfidf_matrix, antique_vectorizer, antique_docs_df,  antique_kmeans)\n",
    "print(f\"clustered TF-IDF Results: {antique_clustered_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TF-IDF System ( Wiki ) ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating TF-IDF System ( Wiki ) ...\")\n",
    "tfidf_wiki_results = evaluate_tfidf_search(wiki_queries, wiki_qrels, wiki_tfidf_matrix, wiki_vectorizer, wiki_docs_df)\n",
    "print(f\"TF-IDF Results: {tfidf_wiki_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating clustered TF-IDF System ( Wiki ) ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating clustered TF-IDF System ( Wiki ) ...\")\n",
    "wiki_clustered_results = evaluate_tfidf_search_with_clustering(wiki_queries, wiki_qrels, wiki_tfidf_matrix, wiki_vectorizer, wiki_docs_df, wiki_kmeans)\n",
    "print(f\"clustered TF-IDF Results: {wiki_clustered_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
